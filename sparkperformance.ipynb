{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50927fb-0580-4d78-ac50-1c5f429f015f",
     "showTitle": true,
     "title": "partition_1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 1\n",
      "After: 5\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file in DBFS\n",
    "path = \"dbfs:/FileStore/tables/file.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame with header option set to True\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"True\").load(path)\n",
    "\n",
    "# Print the number of partitions before repartitioning\n",
    "print(\"Before: {}\".format(df.rdd.getNumPartitions()))\n",
    "\n",
    "# Repartition the DataFrame into 5 partitions\n",
    "df_part = df.repartition(5)\n",
    "\n",
    "# Print the number of partitions after repartitioning\n",
    "print(\"After: {}\".format(df_part.rdd.getNumPartitions()))\n",
    "\n",
    "# Write the original DataFrame to Parquet format in DBFS\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/beforePartition/\")\n",
    "\n",
    "# Write the repartitioned DataFrame to Parquet format in DBFS\n",
    "df_part.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/afterPartition/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66071c26-7852-40ed-9584-c5d06b022652",
     "showTitle": true,
     "title": "partition_2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>_SUCCESS</td><td>0</td></tr><tr><td>_committed_2153051212365741485</td><td>123</td></tr><tr><td>_started_2153051212365741485</td><td>0</td></tr><tr><td>part-00000-tid-2153051212365741485-1123a48a-0782-4ba6-9254-05469e844873-64-1-c000.snappy.parquet</td><td>2007</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "_SUCCESS",
         0
        ],
        [
         "_committed_2153051212365741485",
         123
        ],
        [
         "_started_2153051212365741485",
         0
        ],
        [
         "part-00000-tid-2153051212365741485-1123a48a-0782-4ba6-9254-05469e844873-64-1-c000.snappy.parquet",
         2007
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>_SUCCESS</td><td>0</td></tr><tr><td>_committed_7821724679271796953</td><td>519</td></tr><tr><td>_started_7821724679271796953</td><td>0</td></tr><tr><td>part-00000-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-66-1-c000.snappy.parquet</td><td>1777</td></tr><tr><td>part-00001-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-67-1-c000.snappy.parquet</td><td>1813</td></tr><tr><td>part-00002-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-68-1-c000.snappy.parquet</td><td>1813</td></tr><tr><td>part-00003-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-69-1-c000.snappy.parquet</td><td>1813</td></tr><tr><td>part-00004-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-70-1-c000.snappy.parquet</td><td>1948</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "_SUCCESS",
         0
        ],
        [
         "_committed_7821724679271796953",
         519
        ],
        [
         "_started_7821724679271796953",
         0
        ],
        [
         "part-00000-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-66-1-c000.snappy.parquet",
         1777
        ],
        [
         "part-00001-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-67-1-c000.snappy.parquet",
         1813
        ],
        [
         "part-00002-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-68-1-c000.snappy.parquet",
         1813
        ],
        [
         "part-00003-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-69-1-c000.snappy.parquet",
         1813
        ],
        [
         "part-00004-tid-7821724679271796953-765046ca-26e2-4fbd-97f0-c7ec961182f4-70-1-c000.snappy.parquet",
         1948
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to display file names and sizes\n",
    "def list_file_sizes(path):\n",
    "    # List files and get their sizes\n",
    "    files = dbutils.fs.ls(path)\n",
    "\n",
    "    file_details = []\n",
    "    for file_info in files:\n",
    "        file_details.append((file_info.name, file_info.size))\n",
    "    return file_details\n",
    "\n",
    "# Path to the directory in DBFS before partitioning\n",
    "path = \"dbfs:/FileStore/beforePartition/\"\n",
    "\n",
    "# Get file details (original partition)\n",
    "file_sizes = list_file_sizes(path)\n",
    "display(file_sizes)\n",
    "\n",
    "# Path to the directory in DBFS after partitioning\n",
    "path = \"dbfs:/FileStore/afterPartition/\"\n",
    "\n",
    "# Get file details (partitioned into 5 parts as per the example)\n",
    "file_sizes = list_file_sizes(path)\n",
    "display(file_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39779c7-fa1b-4117-b78c-c76bfdecc9a7",
     "showTitle": true,
     "title": "partition_3"
    }
   },
   "outputs": [],
   "source": [
    "# Partitioning by column:\n",
    "\n",
    "df.repartition(\"column\")\n",
    "\n",
    "# Considerations for Choosing the Column for Partitioning\n",
    "\n",
    "# Column Cardinality:\n",
    "# Low Cardinality: Columns with few distinct values (low cardinality) are good candidates for partitioning because they result in a reasonable number of partitions.\n",
    "# High Cardinality: Columns with many distinct values (high cardinality) can create too many partitions, which can be inefficient.\n",
    "\n",
    "# Frequent Use in Filters:\n",
    "# Choose columns that are frequently used in WHERE, FILTER, or JOIN clauses. This can improve performance because operations can benefit from partitioning.\n",
    "\n",
    "# Data Distribution:\n",
    "# Ensure that the chosen column results in a balanced distribution of data among the partitions. Imbalance can lead to \"hotspots,\" where some partitions are much larger than others, causing performance bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0bfbf9-5ac8-44b6-a890-040edaa6d8d0",
     "showTitle": true,
     "title": "others"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reduce shuffle.\n",
    "\n",
    "Lazy evaluation practices (to give Spark more time to plan the execution), for example: create pointers to temporary views instead of moving data between tables.\n",
    "\n",
    "Check Apache Spark logs!!!\n",
    "\n",
    "Compatibility: check the system that will consume the data after processing and decide the best path. Below are some of the main databases and their best compatibilities:\n",
    "\n",
    "Apache Hive\n",
    "Ideal Format: Parquet\n",
    "Why: Parquet is a columnar storage format optimized for efficient reading and data compression. It is highly compatible with Hive and improves the performance of analytical queries.\n",
    "\n",
    "Amazon Redshift\n",
    "Ideal Format: ORC, Parquet\n",
    "Why: Both formats are columnar and optimized for fast and efficient analytics. They also integrate well with Redshift Spectrum for external queries.\n",
    "\n",
    "Google BigQuery\n",
    "Ideal Format: Avro, Parquet\n",
    "Why: BigQuery is optimized to work with both columnar and row formats, but Parquet and Avro offer efficient compression and improved query performance.\n",
    "\n",
    "Azure Synapse Analytics (formerly SQL Data Warehouse)\n",
    "Ideal Format: Parquet\n",
    "Why: Parquet offers excellent performance with Synapse Analytics due to its optimized columnar format.\n",
    "\n",
    "Snowflake\n",
    "Ideal Format: Parquet, ORC\n",
    "Why: Snowflake is designed to work efficiently with columnar formats like Parquet and ORC, leveraging their compression and fast reading capabilities.\n",
    "\n",
    "Apache Cassandra\n",
    "Ideal Format: CQL (Cassandra Query Language)\n",
    "Why: Cassandra is a NoSQL database that works best with its native CQL query format, which is optimized for fast writes and distributed reads.\n",
    "\n",
    "MongoDB\n",
    "Ideal Format: BSON (Binary JSON)\n",
    "Why: BSON is MongoDB's native format, optimized for efficiently storing JSON documents, including support for rich data types.\n",
    "\n",
    "Presto\n",
    "Ideal Format: Parquet, ORC\n",
    "Why: Presto is a distributed query engine optimized to work with columnar formats, offering fast performance for analytical queries.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sparkperformance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
